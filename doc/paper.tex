\documentclass[twocolumn]{article}

\begin{document}

\title{Tolkienizer}
\author{Sean Anderson \and Sam Payson \and Akanksha Vyas}

\section{Introduction}

Tolkienizer is a program using natural language processing to create artificial words with the sound of a real language.  We use a Hidden Markov Model to learn trends in the language, then attempt to rebuild it using the resulting Markov Chain.  The result is that on small enough word lists, the imprecision of the Markov Chain produces new words with the general form of the language it was trained upon.

\subsection{Natural Language Processing}

Natural Language Processing (NLP) is the broad field of applying computational analysis to language.  It spans a huge variety of sub-fields, from analyzing the structure of individual words to converting text into logic.  For instance, morphology is the problem of breaking a word down into its components, and it operates on a completely different syntactic level from sentence parsing, which breaks down a sentence into its grammatical structures.

\subsection{Tolkienizer Goals}

Our task is somewhat closer to the morphology level of NLP, in that it focuses on words and parts of words.  The Tolkienizer project is an attempt to create fake words that look and sound like they belong to a particular language.  It is named after J.R.R. Tolkien, whose artificial Elvish languages did just that - each form of Elvish was modeled after a different Northern European language.

The Tolkienizer trains on a sample of text from a given language, and learns the patterns of how words are formed in that language.  Then it generates words that it thinks are in the language, but imprecisely, so that it creates words that are not actually in the language, but seem to be.

\section{Implementation}

\subsection{Hidden Markov Models}

\section{Results}

We ran Tolkienizer on three language sets: a collection of writings in Tolkien's Sindarin Elvish language, the GNU English dictionary, and the complete works of Shakespeare.  In the case of Shakespeare, the training set was so large that the HMM became over-trained, and never produced words that were not in the original text.  On the dictionary and Sindarin, however, we had better results.

Sindarin created words that could very easily be mistaken for Elvish, at least by a non-speaker.  Words like ``imlain,'' ``drastannan,'' and ``maethant'' all have a very Tolkien-esque feel, but are not in the original file.  However, with an artificial language like Sindarin, it is probably much easier to make fake words than it is in a language that we are familiar with.

\subsection{Example}

Here is an example using the GNU English dictionary as input:

\begin{verbatim}
$ cat /usr/share/dict/words | ./tolkienizer 
Tolkienizer v0.0
solitholacupriacetamering
spiritter
jockable
two
Coldheromebuillaboviparoo
pyrilelemembertegrous
bethness
nond
Stramadesis
Crest
Acant
burtuidness
papa
chippins
subatter
sub
brodiacarquieting
considicier
leemeiku
flags
\end{verbatim}

\subsection{Analysis}

\section{Conclusion}

\end{document}
